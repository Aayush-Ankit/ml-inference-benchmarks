***mlp_l5 Benchmarking***
Usage ./run_cpu_mlp_l5.sh <numberofthreads> --> e.g: ./run_cpu_mlp_l5.sh 4
**batch size 1**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 23.669004440308ms	
**batch size 16**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 62.707185745239ms	
**batch size 32**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 81.341981887817ms	
**batch size 64**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 78.796148300171ms	
**batch size 128**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 132.29990005493ms	
**batch size 256**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 189.35298919678ms	
**batch size 512**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 213.94920349121ms	
**batch size 1024**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 379.17304039001ms	
**batch size 2048**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 591.85290336609ms	
**batch size 4096**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 881.14213943481ms	
**batch size 8192**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 1651.9000530243ms	
**batch size 16384**
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> output]
  (1): nn.Linear(1024 -> 2048)
  (2): nn.Sigmoid
  (3): nn.Linear(2048 -> 3072)
  (4): nn.Sigmoid
  (5): nn.Linear(3072 -> 3072)
  (6): nn.Sigmoid
  (7): nn.Linear(3072 -> 1024)
  (8): nn.Sigmoid
  (9): nn.Linear(1024 -> 10)
}
20990986	
==> Type is torch.FloatTensor	
CPU Time: 3591.4800167084ms	
